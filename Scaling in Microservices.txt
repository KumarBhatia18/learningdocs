This is called Scale Out (create more instances as the load increases) and Scale In (reduces instances as the load goes down)

That, in essence, is the requirement for auto scaling — a dynamically changing number of microservice instances, and evenly distributing the load across them.

Implementing Auto Scaling

1) Naming Server - Eureka
Naming servers enable something called location transparency. Every microservice registers with the naming service. Any microservice that needs to talk to another microservice will ask the naming server for its location.

Whenever a new instance of CurrencyConversionService or ForexService comes up, it registers with the naming server.

2) Implementing Location Transparency

When to Increase and Decrease Microservices Instances

An application needs to be monitored to find out how much load it has. For this, the application has to expose metrics for us to track the load.

You can containerize each microservice using Docker and create an image.

Kubernetes has the capability to manage containers. Kubernetes can be configured to auto scale based on the load. Kubernetes can identify the application instances, monitor their loads, and automatically scale up and down.


The very nature of DevOps and containers make them inherently more scalable than a traditional IT infrastructure—especially an infrastructure of physical servers in an on-premise data center. The foundation of microservices is built on the cloud and virtualization, both of which include scalability as a core feature. Virtualization enables servers to be created and provisioned at the push of a button as demand spikes, and simply turned off and deleted when the need subsides.

Set up Autoscale settings for your application in the Azure portal
There are two options for Autoscale demand management:

Manual scale: Maintains a fixed instance count. In the Standard tier, you can scale out to a maximum of 500 instances. This value changes the number of separate running instances of the microservice application.
Custom autoscale: Scales on any schedule, based on any metrics.


minikube start \
  --memory 8096 \
  --extra-config=controller-manager.horizontal-pod-autoscaler-upscale-delay=1m \
  --extra-config=controller-manager.horizontal-pod-autoscaler-downscale-delay=2m \
  --extra-config=controller-manager.horizontal-pod-autoscaler-sync-period=10s


First, connect your Docker client to minikube by following the instruction printed by this command:

minikube docker-env
